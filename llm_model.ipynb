{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-27T02:29:40.729747Z",
     "start_time": "2024-10-27T02:29:27.476350Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install --pre -Uq \"openvino>=2024.2.0\" openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
    "\"torch>=2.1\"\\\n",
    "\"datasets\" \\\n",
    "\"accelerate\" \\\n",
    "\"gradio>=4.19\" \\\n",
    "\"onnx<=1.16.1; sys_platform=='win32'\" \"einops\" \"transformers>=4.43.1\" \"transformers_stream_generator\" \"tiktoken\" \"bitsandbytes\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001B[33mWARNING: Skipping optimum as it is not installed.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Skipping optimum-intel as it is not installed.\u001B[0m\u001B[33m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: openvino-tokenizers[transformers]\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement torch>=2.1 (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for torch>=2.1\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:33:10.913256Z",
     "start_time": "2024-10-27T02:33:10.647598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# fetch model configuration\n",
    "\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)"
   ],
   "id": "f8ed736aac16be19",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:43:31.764756Z",
     "start_time": "2024-10-27T02:43:30.540994Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install ipywidgets",
   "id": "c7f6cee8cdbf0728",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\r\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in ./openvino_env/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./openvino_env/lib/python3.13/site-packages (from ipywidgets) (8.29.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./openvino_env/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\r\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\r\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\r\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: decorator in ./openvino_env/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in ./openvino_env/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in ./openvino_env/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./openvino_env/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./openvino_env/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\r\n",
      "Requirement already satisfied: stack-data in ./openvino_env/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: pexpect>4.3 in ./openvino_env/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./openvino_env/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./openvino_env/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in ./openvino_env/lib/python3.13/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in ./openvino_env/lib/python3.13/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./openvino_env/lib/python3.13/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in ./openvino_env/lib/python3.13/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./openvino_env/lib/python3.13/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\r\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\r\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\r\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.3/2.3 MB\u001B[0m \u001B[31m14.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\r\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:43:35.576247Z",
     "start_time": "2024-10-27T02:43:35.543391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llm_config import SUPPORTED_LLM_MODELS\n",
    "import ipywidgets as widgets"
   ],
   "id": "a746b8b1a71ca8b2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:44:09.365491Z",
     "start_time": "2024-10-27T02:44:09.355889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_languages = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "model_language = widgets.Dropdown(\n",
    "    options=model_languages,\n",
    "    value=model_languages[0],\n",
    "    description=\"Model Language:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_language"
   ],
   "id": "6aba75e27dc36b90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropdown(description='Model Language:', options=('English', 'Chinese', 'Japanese'), value='English')"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bfb5e9c64a6461a925f53e8a07fe487"
      }
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:44:39.499720Z",
     "start_time": "2024-10-27T02:44:39.492460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_ids = list(SUPPORTED_LLM_MODELS[model_language.value])\n",
    "\n",
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids,\n",
    "    value=model_ids[0],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ],
   "id": "4329c346097ea0d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropdown(description='Model:', options=('qwen2.5-0.5b-instruct', 'tiny-llama-1b-chat', 'llama-3.2-1b-instruct'…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad3c9d5e16a24e80a481e98319d8c843"
      }
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:46:19.171561Z",
     "start_time": "2024-10-27T02:46:19.166661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_configuration = SUPPORTED_LLM_MODELS[model_language.value][model_id.value]\n",
    "print(f\"Selected model {model_id.value}\")"
   ],
   "id": "d2eea91885721fa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model neural-chat-7b-v3-3\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:46:48.773695Z",
     "start_time": "2024-10-27T02:46:48.762957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "prepare_int4_model = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int8_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_fp16_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare FP16 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(prepare_int4_model)\n",
    "display(prepare_int8_model)\n",
    "display(prepare_fp16_model)"
   ],
   "id": "d81f55d25c41b1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Checkbox(value=True, description='Prepare INT4 model')"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c94843fe8b4641aa8242a41c58c5d8da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Checkbox(value=False, description='Prepare INT8 model')"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "544cd295b7c2442ba8e6bde750feba3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Checkbox(value=False, description='Prepare FP16 model')"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2afe889ea794a4484cbd0d680236242"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:47:11.773926Z",
     "start_time": "2024-10-27T02:47:11.766089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enable_awq = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Enable AWQ\",\n",
    "    disabled=not prepare_int4_model.value,\n",
    ")\n",
    "display(enable_awq)"
   ],
   "id": "8ea30d761a9c4e39",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Checkbox(value=False, description='Enable AWQ')"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fed9889ed7641dcb5806113d81e4d85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:48:09.621402Z",
     "start_time": "2024-10-27T02:48:04.773794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "pt_model_id = model_configuration[\"model_id\"]\n",
    "pt_model_name = model_id.value.split(\"-\")[0]\n",
    "fp16_model_dir = Path(model_id.value) / \"FP16\"\n",
    "int8_model_dir = Path(model_id.value) / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(model_id.value) / \"INT4_compressed_weights\"\n",
    "\n",
    "\n",
    "def convert_to_fp16():\n",
    "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format fp16\".format(pt_model_id)\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(fp16_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int8\".format(pt_model_id)\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int8_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "def convert_to_int4():\n",
    "    compression_configs = {\n",
    "        \"zephyr-7b-beta\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"mistral-7b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"minicpm-2b-dpo\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"gemma-2b-it\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"notus-7b-v1\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"neural-chat-7b-v3-1\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"llama-2-chat-7b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"llama-3-8b-instruct\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"llama-3.1-8b-instruct\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 1.0,\n",
    "        },\n",
    "        \"gemma-7b-it\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"chatglm2-6b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.72,\n",
    "        },\n",
    "        \"qwen-7b-chat\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.6},\n",
    "        \"red-pajama-3b-chat\": {\n",
    "            \"sym\": False,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.5,\n",
    "        },\n",
    "        \"qwen2.5-7b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-3b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-14b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-1.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-0.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"default\": {\n",
    "            \"sym\": False,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    model_compression_params = compression_configs.get(model_id.value, compression_configs[\"default\"])\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int4\".format(pt_model_id)\n",
    "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
    "    if model_compression_params[\"sym\"]:\n",
    "        int4_compression_args += \" --sym\"\n",
    "    if enable_awq.value:\n",
    "        int4_compression_args += \" --awq --dataset wikitext2 --num-samples 128\"\n",
    "    export_command_base += int4_compression_args\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int4_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "if prepare_fp16_model.value:\n",
    "    convert_to_fp16()\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()\n",
    "if prepare_int4_model.value:\n",
    "    convert_to_int4()"
   ],
   "id": "68cb8ddb397af940",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "**Export command:**"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "`optimum-cli export openvino --model Intel/neural-chat-7b-v3-3 --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.8 neural-chat-7b-v3-3/INT4_compressed_weights`"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/abbottubeine/miniconda3/bin/optimum-cli\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "             ^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/optimum/commands/optimum_cli.py\", line 208, in main\r\n",
      "    service.run()\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/optimum/commands/export/openvino.py\", line 250, in run\r\n",
      "    library_name = _infer_library_from_model_name_or_path(\r\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/optimum/intel/utils/modeling_utils.py\", line 258, in _infer_library_from_model_name_or_path\r\n",
      "    library_name = TasksManager._infer_library_from_model_name_or_path(\r\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/optimum/exporters/tasks.py\", line 1852, in _infer_library_from_model_name_or_path\r\n",
      "    config_dict, kwargs = PretrainedConfig.get_config_dict(model_name_or_path, **kwargs)\r\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 570, in get_config_dict\r\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 629, in _get_config_dict\r\n",
      "    resolved_config_file = cached_file(\r\n",
      "                           ^^^^^^^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py\", line 403, in cached_file\r\n",
      "    resolved_file = hf_hub_download(\r\n",
      "                    ^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n",
      "    return fn(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\r\n",
      "    return _hf_hub_download_to_cache_dir(\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1009, in _hf_hub_download_to_cache_dir\r\n",
      "    Path(lock_path).parent.mkdir(parents=True, exist_ok=True)\r\n",
      "  File \"/Users/abbottubeine/miniconda3/lib/python3.12/pathlib.py\", line 1311, in mkdir\r\n",
      "    os.mkdir(self, mode)\r\n",
      "PermissionError: [Errno 13] Permission denied: '/Users/abbottubeine/.cache/huggingface/hub/.locks/models--Intel--neural-chat-7b-v3-3'\r\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:50:17.627105Z",
     "start_time": "2024-10-27T02:50:17.111742Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install openvino",
   "id": "f3e91a1a75e285e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not find a version that satisfies the requirement openvino (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for openvino\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T02:49:49.189328Z",
     "start_time": "2024-10-27T02:49:47.611571Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install numpy",
   "id": "537723f98f53e127",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\r\n",
      "  Using cached numpy-2.1.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (60 kB)\r\n",
      "Using cached numpy-2.1.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\r\n",
      "Installing collected packages: numpy\r\n",
      "Successfully installed numpy-2.1.2\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T03:11:39.007031Z",
     "start_time": "2024-10-27T03:11:38.700223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget(\"CPU\", exclude=[\"NPU\"])\n",
    "\n",
    "device"
   ],
   "id": "a32d5803885c50ef",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openvino'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 8\u001B[0m\n\u001B[1;32m      3\u001B[0m r \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m      4\u001B[0m     url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m )\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnotebook_utils.py\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite(r\u001B[38;5;241m.\u001B[39mtext)\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnotebook_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m device_widget\n\u001B[1;32m     10\u001B[0m device \u001B[38;5;241m=\u001B[39m device_widget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCPU\u001B[39m\u001B[38;5;124m\"\u001B[39m, exclude\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNPU\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     12\u001B[0m device\n",
      "File \u001B[0;32m~/Desktop/CodeArea/PORTFOLIO PROJECTS/hackathons/notebook_utils.py:18\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List, NamedTuple, Optional, Tuple\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenvino\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mruntime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Core, Type, get_version\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTML, Image, display\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mopenvino\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mov\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'openvino'"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
